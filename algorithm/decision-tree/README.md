# 决策树

#### 最小分割样本数

#### 信息熵(Entropy)

> 描述信息的不确定性，信息的不确定性越大，熵越大

<div align="center"><img src="http://latex.codecogs.com/svg.latex?E(x)=-\sum_{k=1}^{c}p_{k}log_{2}p_{k}" /></a></div>

* 只有一种类别时，熵的值最小，为0
* 每种类别出现的概率相同时，熵的值最小，为<img src="http://latex.codecogs.com/svg.latex?log_{2}N" /></a>

#### 信息增益(Information Gain)

#### 偏差(Bias)和方差(Variance)

#### 特征数

> 特征数越多，过拟合的可能性越大

#### 复杂度

> 特征数越多，复杂度越大
